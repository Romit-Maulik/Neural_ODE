{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Jax_NODE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iO00BLAEVF9I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import jax.numpy as np\n",
        "from jax import grad, jit, vmap, jacrev\n",
        "from jax import random\n",
        "from jax import ops\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as onp\n",
        "from jax import device_put"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jw9ZbBdVJOF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tsteps = 2000\n",
        "batch_tsteps = 10\n",
        "num_batches = 10\n",
        "dt = 25.0/tsteps\n",
        "reg_param = 0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwZ4JheHVrlc",
        "colab_type": "code",
        "outputId": "efe8db60-cfff-4fd2-ed7a-40a057e996ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "# Generating dynamical system data\n",
        "def true_forward_solver(state,ds_mat):\n",
        "    i0 = state + dt*np.matmul(state,ds_mat)\n",
        "    return i0, np.matmul(i0,ds_mat)\n",
        "\n",
        "true_forward_solver = jit(true_forward_solver) # Just in time compile\n",
        "\n",
        "# State (z), rhs saver (f)\n",
        "state_len = 2\n",
        "true_state_array = np.zeros(shape=(tsteps,state_len)) # No double in Jax and mutable numpy\n",
        "true_rhs_array = np.zeros(shape=(tsteps,state_len)) # No double in Jax and mutable numpy\n",
        "# Time array - fixed\n",
        "time_array = dt*np.arange(tsteps)\n",
        "\n",
        "# DS definition\n",
        "init_state = np.asarray([[2.0,0.0]])\n",
        "ds_mat = np.asarray([[-0.1, 2.0], [-2.0, -0.1]])\n",
        "\n",
        "true_state_array = ops.index_update(true_state_array, ops.index[0:1,:], init_state)\n",
        "true_rhs_array = ops.index_update(true_rhs_array, ops.index[0:1,:], np.matmul(init_state,ds_mat))\n",
        "\n",
        "\n",
        "for i in range(1,tsteps):\n",
        "    state_new, rhs_new = true_forward_solver(init_state,ds_mat)\n",
        "\n",
        "    true_state_array = ops.index_update(true_state_array, ops.index[i:i+1,:], state_new)\n",
        "    true_rhs_array = ops.index_update(true_rhs_array, ops.index[i:i+1,:], np.matmul(init_state,ds_mat))\n",
        "    init_state = ops.index_update(init_state, ops.index[:], state_new)\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(true_state_array[:,0])\n",
        "plt.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBZ9hGKZCYMFnGIapEP4fOgR2oY3HtKMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W05n2OWIV1fv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define neural network parameters - simple 1-layer fc,ff nn - Xavier initialization\n",
        "num_neurons = 30\n",
        "weights_1 = onp.random.randn(state_len,num_neurons)*onp.sqrt(1.0/(state_len+num_neurons)) \n",
        "weights_2 = onp.random.randn(num_neurons,state_len)*onp.sqrt(1.0/(state_len+num_neurons))  \n",
        "bias_1 = onp.random.randn(1,num_neurons)*onp.sqrt(1.0/(num_neurons))\n",
        "bias_2 = onp.random.randn(1,state_len)*onp.sqrt(1.0/(state_len))\n",
        "# Flatten (and reshape) parameters for the purpose of autograd\n",
        "thetas = onp.concatenate((weights_1.flatten(),bias_1.flatten(),weights_2.flatten(),bias_2.flatten()),axis=0)\n",
        "num_wb = onp.shape(thetas)[0]\n",
        "thetas = onp.reshape(thetas,newshape=(1,num_wb))\n",
        "\n",
        "thetas = device_put(thetas) # Put weights on device\n",
        "\n",
        "# Reshaping indices\n",
        "w1_idx_end = num_neurons*(state_len) # not inclusive\n",
        "b1_idx_start = num_neurons*(state_len)\n",
        "b1_idx_end = num_neurons*(state_len) + num_neurons\n",
        "\n",
        "w2_idx_start = num_neurons*(state_len) + num_neurons\n",
        "w2_idx_end = num_neurons*(state_len) + num_neurons + num_neurons*state_len\n",
        "\n",
        "b2_idx_start = num_neurons*(state_len) + num_neurons + num_neurons*state_len\n",
        "\n",
        "# Reshaping function for parameters\n",
        "def theta_reshape(thetas):\n",
        "    weights_1_flattened = thetas[0,0:w1_idx_end]\n",
        "    bias_1_flattened = thetas[0,b1_idx_start:b1_idx_end]\n",
        "\n",
        "    weights_2_flattened = thetas[0,w2_idx_start:w2_idx_end]\n",
        "    bias_2_flattened = thetas[0,b2_idx_start:]\n",
        "\n",
        "    weights_1 = np.reshape(weights_1_flattened,(state_len,num_neurons))\n",
        "    weights_2 = np.reshape(weights_2_flattened,(num_neurons,state_len))\n",
        "\n",
        "    bias_1 = np.reshape(bias_1_flattened,(1,num_neurons))\n",
        "    bias_2 = np.reshape(bias_2_flattened,(1,state_len))\n",
        "\n",
        "    return weights_1, weights_2, bias_1, bias_2\n",
        "\n",
        "# Simple Feed forward network for RHS calculation\n",
        "def ffnn(state,weights_1,weights_2,bias_1,bias_2):\n",
        "    h = np.tanh(np.matmul(state,weights_1)+bias_1)\n",
        "    return np.matmul(h,weights_2)+bias_2\n",
        "\n",
        "\n",
        "theta_reshape = jit(theta_reshape) # Just in time compile\n",
        "ffnn = jit(ffnn) # Just in time compile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDtJvCLYXkVQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Our ODE integrator - Euler Forward\n",
        "# Forward model (Neural ODE formulation) - one timestep\n",
        "def euler_forward(state,weights_1,weights_2,bias_1,bias_2,time):\n",
        "    # Step 1\n",
        "    i0 = state + dt*ffnn(state,weights_1,weights_2,bias_1,bias_2)\n",
        "    # Output for saving state\n",
        "    f_rhs = ffnn(i0,weights_1,weights_2,bias_1,bias_2)\n",
        "\n",
        "    return i0, f_rhs\n",
        "\n",
        "euler_forward = jit(euler_forward) # Just in time compile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6awN6EMXuVo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The final loss functions - this is needed for adjoint initial condition calculation\n",
        "# First as a function of final state (dldz)\n",
        "def training_loss_state(state,true_state):\n",
        "    loss = np.sum((state - true_state)**2)\n",
        "    return loss\n",
        "\n",
        "training_loss_state = jit(training_loss_state) # Just in time compile\n",
        "\n",
        "# Its gradient\n",
        "dldz_func = jit(grad(training_loss_state,0)) # Output is a (1,state_len) array\n",
        "# Secondly as a function of pvec (a concatenated vector of state,thetas,time)\n",
        "def training_loss(pvec,true_state):\n",
        "    _state = np.reshape(pvec[:,:state_len],(1,state_len)) # The state\n",
        "    _thetas = np.reshape(pvec[:,state_len:-1],(1,num_wb)) # NN Parameters\n",
        "    _time = np.reshape(pvec[:,-1],(1,1)) # Time\n",
        "    _weights_1, _weights_2, _bias_1, _bias_2 = theta_reshape(_thetas)\n",
        "    _output_state, _ = euler_forward(_state,_weights_1,_weights_2, _bias_1, _bias_2,_time)\n",
        "    loss = np.sum((_output_state - true_state)**2)\n",
        "    return loss\n",
        "\n",
        "training_loss = jit(training_loss) # Just in time compile  \n",
        "# Its gradient\n",
        "dl_func = jit(grad(training_loss,0)) # Output is a (1,state_len+num_wb+1) array\n",
        "# Now we define the parameterization (i.e., the rhs of the ODE in terms of an NN - f(z))\n",
        "def rhs_calculator(pvec):\n",
        "    _state = np.reshape(pvec[:,:state_len],(1,state_len)) # The state\n",
        "    _thetas = np.reshape(pvec[:,state_len:-1],(1,num_wb)) # NN Parameters\n",
        "    _time = np.reshape(pvec[:,-1],(1,1)) # Time    \n",
        "    _weights_1, _weights_2, _bias_1, _bias_2 = theta_reshape(_thetas)\n",
        "    rhs = ffnn(_state,_weights_1,_weights_2, _bias_1, _bias_2)\n",
        "    rhs = rhs.flatten()\n",
        "    return rhs\n",
        "\n",
        "rhs_calculator = jit(rhs_calculator) # Just in time compile  \n",
        "# Calculate Jacobians - dfdz, dfdthetas, dfdt\n",
        "df_func = jit(jacrev(rhs_calculator)) # Output is a (state_len,1,state_len+num_wb+1) array - must be squeezed as needed\n",
        "# Adjoint RHS calculation\n",
        "def adjoint_rhs(a,pvec):\n",
        "    # Time to calculate Jacobians\n",
        "    df = np.squeeze(df_func(pvec),axis=1)\n",
        "    dfdz = df[:,0:state_len]\n",
        "\n",
        "    dfdthetas = df[:,state_len:-1]\n",
        "    dfdt = np.reshape(df[:,-1],(state_len,1))\n",
        "\n",
        "    dfs = np.concatenate((dfdz,dfdthetas,dfdt),axis=1)\n",
        "    rhs_reverse = np.matmul(a[:,:state_len],dfs)\n",
        "\n",
        "    return rhs_reverse\n",
        "\n",
        "adjoint_rhs = jit(adjoint_rhs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmqoctE5BpqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Neural ODE algorithm - minibatching\n",
        "def neural_ode(thetas):\n",
        "    weights_1, weights_2, bias_1, bias_2 = theta_reshape(thetas) # Reshape once for utilization in entire iteration\n",
        "    batch_state_array = np.zeros(shape=(num_batches,batch_tsteps,state_len),dtype='double') # \n",
        "    batch_rhs_array = np.zeros(shape=(num_batches,batch_tsteps,state_len),dtype='double') #\n",
        "    batch_time_array = np.zeros(shape=(num_batches,batch_tsteps,1),dtype='double') #\n",
        "\n",
        "    augmented_state = np.zeros(shape=(1,state_len+num_wb+1))\n",
        "    batch_ids = onp.random.choice(tsteps-batch_tsteps,num_batches)\n",
        "    batch_ids = device_put(batch_ids)\n",
        "\n",
        "    # Minibatching within sampled domain\n",
        "    total_batch_loss = 0.0\n",
        "    for j in range(num_batches):\n",
        "        start_id = batch_ids[j]\n",
        "        end_id = start_id + batch_tsteps\n",
        "\n",
        "        # batch_state_array[j,0,:] = true_state_array[start_id,:]\n",
        "        batch_state_array = ops.index_update(batch_state_array, ops.index[j:j+1,0:1,:], true_state_array[start_id,:])\n",
        "\n",
        "\n",
        "        # batch_rhs_array[j,0,:] = true_rhs_array[start_id,:]\n",
        "        batch_rhs_array = ops.index_update(batch_rhs_array, ops.index[j:j+1,0:1,:], true_rhs_array[start_id,:])\n",
        "\n",
        "        # batch_time_array[j,:batch_tsteps] = time_array[start_id:end_id,None]\n",
        "        batch_time_array = ops.index_update(batch_time_array, ops.index[j:j+1,:batch_tsteps], time_array[start_id:end_id,None])\n",
        "\n",
        "        # Calculate forward pass - saving results for state and rhs to array - batchwise\n",
        "        temp_state = batch_state_array[j,0,:]\n",
        "        for i in range(1,batch_tsteps):\n",
        "            time = np.reshape(batch_time_array[j,i],newshape=(1,1))\n",
        "            output_state, output_rhs = euler_forward(temp_state,weights_1,weights_2,bias_1,bias_2,time)  \n",
        "            \n",
        "            # batch_state_array[j,i,:] = output_state[:]\n",
        "            batch_state_array = ops.index_update(batch_state_array, ops.index[j:j+1,i:i+1,:], output_state[:])\n",
        "\n",
        "            # batch_rhs_array[j,i,:] = output_rhs[:]\n",
        "            batch_rhs_array = ops.index_update(batch_rhs_array, ops.index[j:j+1,i:i+1,:], output_rhs[:])\n",
        "\n",
        "            temp_state = output_state\n",
        "\n",
        "        # Operations at final time step (setting up initial conditions for the adjoint)\n",
        "        temp_state = batch_state_array[j,-2,:]\n",
        "        temp_state = np.reshape(temp_state,(1,state_len)) # prefinal state vector\n",
        "        time = np.reshape(batch_time_array[j,-2],(1,1)) # prefinal time\n",
        "        pvec = np.concatenate((temp_state,thetas,time),axis=1)\n",
        "\n",
        "        # Calculate loss related gradients - dldz\n",
        "        dldz = np.reshape(dldz_func(output_state,true_state_array[end_id-1,:]),(1,state_len))\n",
        "        # With respect to weights,bias and time\n",
        "        dl = dl_func(pvec,true_state_array[end_id-1,:])\n",
        "        dldthetas = onp.reshape(dl[:,state_len:-1],newshape=(1,num_wb))\n",
        "        # Calculate dl/dt\n",
        "        dldt = np.matmul(dldz,batch_rhs_array[j,-1,:])\n",
        "        dldt = np.reshape(dldt,newshape=(1,1))\n",
        "\n",
        "        # Find batch loss\n",
        "        total_batch_loss = total_batch_loss + np.sum((output_state-true_state_array[end_id-1,:])**2)\n",
        "\n",
        "        # Reverse operation (adjoint evolution in backward time)\n",
        "        _augmented_state = np.concatenate((dldz,dldthetas,dldt),axis=1)\n",
        "        for i in range(1,batch_tsteps):\n",
        "            time = np.reshape(batch_time_array[j,-1-i],newshape=(1,1))\n",
        "            state_now = np.reshape(batch_state_array[j,-1-i,:],newshape=(1,state_len))\n",
        "            pvec = np.concatenate((state_now,thetas,time),axis=1)\n",
        "\n",
        "            # Adjoint propagation backward in time\n",
        "            i0 = _augmented_state + dt*adjoint_rhs(_augmented_state,pvec)\n",
        "            sub_state = np.reshape(i0[0,:state_len],newshape=(1,state_len))\n",
        "\n",
        "            _augmented_state = i0\n",
        "        \n",
        "        augmented_state = np.add(augmented_state,_augmented_state)\n",
        "    \n",
        "    return augmented_state, total_batch_loss\n",
        "\n",
        "neural_ode = jit(neural_ode)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3Zxv1oBCJOT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Optimization\n",
        "def rms_prop_optimize(thetas):\n",
        "    num_epochs = 200\n",
        "    lr = 0.01\n",
        "    beta = 0.9\n",
        "    lr_counter = 0\n",
        "    exp_gradient = np.zeros(shape=(1,num_wb))\n",
        "    best_loss = onp.Inf\n",
        "    loss_list = []\n",
        "    for epoch in range(num_epochs):\n",
        "        augmented_state, total_batch_loss = neural_ode(thetas)      \n",
        "              \n",
        "        if epoch == 0:\n",
        "            exp_gradient = (1.0-beta)*(augmented_state[0,state_len:-1]**2)\n",
        "        else:\n",
        "            exp_gradient = beta*exp_gradient + (1.0-beta)*(augmented_state[0,state_len:-1]**2)\n",
        "\n",
        "        thetas = thetas - lr/(np.sqrt(exp_gradient))*(augmented_state[0,state_len:-1])\n",
        "        lr_counter = lr_counter + 1\n",
        "\n",
        "        if total_batch_loss<best_loss:\n",
        "            np.save('Trained_Weights.npy',thetas)\n",
        "            # visualize(mode='train')\n",
        "            best_loss = total_batch_loss\n",
        "\n",
        "        print('iteration: ',epoch,' Loss: ',best_loss)\n",
        "        loss_list.append(total_batch_loss)\n",
        "        \n",
        "        if lr_counter > 100:\n",
        "            lr = lr*0.9\n",
        "            lr_counter = 0\n",
        "\n",
        "    return thetas, loss_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ycy1AZYC017",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualization function\n",
        "def visualize(mode='train'):\n",
        "    # Visualization fluff here\n",
        "    fig, ax = plt.subplots(nrows=2,ncols=1)\n",
        "\n",
        "    ax[0].set_title('Mode 1')\n",
        "    ax[0].set_ylim(-4.0,4.0)\n",
        "    ax[1].set_title('Mode 2')\n",
        "    ax[1].set_ylim(-4.0,4.0)\n",
        "\n",
        "    t1n1 = ax[0].plot(true_state_array[:,0],label='True')\n",
        "    t1n2 = ax[1].plot(true_state_array[:,1],label='True')\n",
        "    predicted_states = forward_model()\n",
        "    # Visualization of modal evolution\n",
        "    ln1, = ax[0].plot(predicted_states[:,0],label='ML',color='orange')\n",
        "    ln2, = ax[1].plot(predicted_states[:,1],label='ML',color='orange')\n",
        "\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Forward model\n",
        "def forward_model():\n",
        "    thetas = np.load('Trained_Weights.npy')\n",
        "    weights_1, weights_2, bias_1, bias_2 = theta_reshape(thetas)\n",
        "\n",
        "    # Calculate forward pass - saving results for state and rhs to array\n",
        "    pred_state_array = np.zeros(shape=(tsteps,state_len),dtype='double')\n",
        "\n",
        "    pred_state_array = ops.index_update(pred_state_array, ops.index[0:1,:], true_state_array[0,:])\n",
        "    temp_state = true_state_array[0,:]\n",
        "    for i in range(1,tsteps):\n",
        "        time = np.reshape(time_array[i],(1,1))\n",
        "        output_state, output_rhs = euler_forward(temp_state,weights_1,weights_2,bias_1,bias_2,time)\n",
        "        pred_state_array = ops.index_update(pred_state_array, ops.index[i:i+1,:], output_state[:])\n",
        "        temp_state = output_state\n",
        "\n",
        "    return pred_state_array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROHWB-KPCOyv",
        "colab_type": "code",
        "outputId": "e1cbfe80-0215-4970-9694-079e63ab806f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "thetas_optimal, loss_list = rms_prop_optimize(thetas)\n",
        "print('Total_time_taken:',time.time()-start_time)\n",
        "\n",
        "np.save('Trained_Weights.npy',thetas_optimal)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/jax/lax/lax.py:4578: UserWarning: Explicitly requested dtype double requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  warnings.warn(msg.format(dtype, fun_name , truncated_dtype))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration:  0  Loss:  0.73724437\n",
            "iteration:  1  Loss:  0.53621167\n",
            "iteration:  2  Loss:  0.41269293\n",
            "iteration:  3  Loss:  0.31922403\n",
            "iteration:  4  Loss:  0.24565884\n",
            "iteration:  5  Loss:  0.18742542\n",
            "iteration:  6  Loss:  0.14157689\n",
            "iteration:  7  Loss:  0.105817676\n",
            "iteration:  8  Loss:  0.07823747\n",
            "iteration:  9  Loss:  0.057220656\n",
            "iteration:  10  Loss:  0.041404217\n",
            "iteration:  11  Loss:  0.029645607\n",
            "iteration:  12  Loss:  0.021005502\n",
            "iteration:  13  Loss:  0.014728169\n",
            "iteration:  14  Loss:  0.010222637\n",
            "iteration:  15  Loss:  0.007034554\n",
            "iteration:  16  Loss:  0.0048325686\n",
            "iteration:  17  Loss:  0.0034078665\n",
            "iteration:  18  Loss:  0.0027313998\n",
            "iteration:  19  Loss:  0.0024525223\n",
            "iteration:  20  Loss:  0.001975395\n",
            "iteration:  21  Loss:  0.0014193471\n",
            "iteration:  22  Loss:  0.0010770496\n",
            "iteration:  23  Loss:  0.00090577384\n",
            "iteration:  24  Loss:  0.0008400745\n",
            "iteration:  25  Loss:  0.0008400745\n",
            "iteration:  26  Loss:  0.0008400745\n",
            "iteration:  27  Loss:  0.0008400745\n",
            "iteration:  28  Loss:  0.0008400745\n",
            "iteration:  29  Loss:  0.0008400745\n",
            "iteration:  30  Loss:  0.0008400745\n",
            "iteration:  31  Loss:  0.0008400745\n",
            "iteration:  32  Loss:  0.0008400745\n",
            "iteration:  33  Loss:  0.0008400745\n",
            "iteration:  34  Loss:  0.0008400745\n",
            "iteration:  35  Loss:  0.0008400745\n",
            "iteration:  36  Loss:  0.0008400745\n",
            "iteration:  37  Loss:  0.0008400745\n",
            "iteration:  38  Loss:  0.0008400745\n",
            "iteration:  39  Loss:  0.0008400745\n",
            "iteration:  40  Loss:  0.0008400745\n",
            "iteration:  41  Loss:  0.0008400745\n",
            "iteration:  42  Loss:  0.0008400745\n",
            "iteration:  43  Loss:  0.0008400745\n",
            "iteration:  44  Loss:  0.0008400745\n",
            "iteration:  45  Loss:  0.0008400745\n",
            "iteration:  46  Loss:  0.0008400745\n",
            "iteration:  47  Loss:  0.0008400745\n",
            "iteration:  48  Loss:  0.0008400745\n",
            "iteration:  49  Loss:  0.0008400745\n",
            "iteration:  50  Loss:  0.0008400745\n",
            "iteration:  51  Loss:  0.0008400745\n",
            "iteration:  52  Loss:  0.0008400745\n",
            "iteration:  53  Loss:  0.0008400745\n",
            "iteration:  54  Loss:  0.0008400745\n",
            "iteration:  55  Loss:  0.0008400745\n",
            "iteration:  56  Loss:  0.0008400745\n",
            "iteration:  57  Loss:  0.0008400745\n",
            "iteration:  58  Loss:  0.0008400745\n",
            "iteration:  59  Loss:  0.0008400745\n",
            "iteration:  60  Loss:  0.0008400745\n",
            "iteration:  61  Loss:  0.0008400745\n",
            "iteration:  62  Loss:  0.0008400745\n",
            "iteration:  63  Loss:  0.0008400745\n",
            "iteration:  64  Loss:  0.0008400745\n",
            "iteration:  65  Loss:  0.0008400745\n",
            "iteration:  66  Loss:  0.0008400745\n",
            "iteration:  67  Loss:  0.0008400745\n",
            "iteration:  68  Loss:  0.0008400745\n",
            "iteration:  69  Loss:  0.0008400745\n",
            "iteration:  70  Loss:  0.0008400745\n",
            "iteration:  71  Loss:  0.0008400745\n",
            "iteration:  72  Loss:  0.0008400745\n",
            "iteration:  73  Loss:  0.0008400745\n",
            "iteration:  74  Loss:  0.0008400745\n",
            "iteration:  75  Loss:  0.0008400745\n",
            "iteration:  76  Loss:  0.0008400745\n",
            "iteration:  77  Loss:  0.0008400745\n",
            "iteration:  78  Loss:  0.0008400745\n",
            "iteration:  79  Loss:  0.0008400745\n",
            "iteration:  80  Loss:  0.0008400745\n",
            "iteration:  81  Loss:  0.0008400745\n",
            "iteration:  82  Loss:  0.0008400745\n",
            "iteration:  83  Loss:  0.0008400745\n",
            "iteration:  84  Loss:  0.0008400745\n",
            "iteration:  85  Loss:  0.0008400745\n",
            "iteration:  86  Loss:  0.0008400745\n",
            "iteration:  87  Loss:  0.0008400745\n",
            "iteration:  88  Loss:  0.0008400745\n",
            "iteration:  89  Loss:  0.0008400745\n",
            "iteration:  90  Loss:  0.0008400745\n",
            "iteration:  91  Loss:  0.0008400745\n",
            "iteration:  92  Loss:  0.0008400745\n",
            "iteration:  93  Loss:  0.0008400745\n",
            "iteration:  94  Loss:  0.0008400745\n",
            "iteration:  95  Loss:  0.0008400745\n",
            "iteration:  96  Loss:  0.0008400745\n",
            "iteration:  97  Loss:  0.0008400745\n",
            "iteration:  98  Loss:  0.0008400745\n",
            "iteration:  99  Loss:  0.0008400745\n",
            "iteration:  100  Loss:  0.0008400745\n",
            "iteration:  101  Loss:  0.0008400745\n",
            "iteration:  102  Loss:  0.0008400745\n",
            "iteration:  103  Loss:  0.0008400745\n",
            "iteration:  104  Loss:  0.0008400745\n",
            "iteration:  105  Loss:  0.0008400745\n",
            "iteration:  106  Loss:  0.0008400745\n",
            "iteration:  107  Loss:  0.0008400745\n",
            "iteration:  108  Loss:  0.0008400745\n",
            "iteration:  109  Loss:  0.0008400745\n",
            "iteration:  110  Loss:  0.0008400745\n",
            "iteration:  111  Loss:  0.0008400745\n",
            "iteration:  112  Loss:  0.0008400745\n",
            "iteration:  113  Loss:  0.0008400745\n",
            "iteration:  114  Loss:  0.0008400745\n",
            "iteration:  115  Loss:  0.0008400745\n",
            "iteration:  116  Loss:  0.0008400745\n",
            "iteration:  117  Loss:  0.0008400745\n",
            "iteration:  118  Loss:  0.0008400745\n",
            "iteration:  119  Loss:  0.0008400745\n",
            "iteration:  120  Loss:  0.0008400745\n",
            "iteration:  121  Loss:  0.0008400745\n",
            "iteration:  122  Loss:  0.0008400745\n",
            "iteration:  123  Loss:  0.0008400745\n",
            "iteration:  124  Loss:  0.0008400745\n",
            "iteration:  125  Loss:  0.0008400745\n",
            "iteration:  126  Loss:  0.0008400745\n",
            "iteration:  127  Loss:  0.0008400745\n",
            "iteration:  128  Loss:  0.0008400745\n",
            "iteration:  129  Loss:  0.0008400745\n",
            "iteration:  130  Loss:  0.0008400745\n",
            "iteration:  131  Loss:  0.0008400745\n",
            "iteration:  132  Loss:  0.0008400745\n",
            "iteration:  133  Loss:  0.0008400745\n",
            "iteration:  134  Loss:  0.0008400745\n",
            "iteration:  135  Loss:  0.0008400745\n",
            "iteration:  136  Loss:  0.0008400745\n",
            "iteration:  137  Loss:  0.0008400745\n",
            "iteration:  138  Loss:  0.0008400745\n",
            "iteration:  139  Loss:  0.0008400745\n",
            "iteration:  140  Loss:  0.0008400745\n",
            "iteration:  141  Loss:  0.0008400745\n",
            "iteration:  142  Loss:  0.0008400745\n",
            "iteration:  143  Loss:  0.0008400745\n",
            "iteration:  144  Loss:  0.0008400745\n",
            "iteration:  145  Loss:  0.0008400745\n",
            "iteration:  146  Loss:  0.0008400745\n",
            "iteration:  147  Loss:  0.0008400745\n",
            "iteration:  148  Loss:  0.0008400745\n",
            "iteration:  149  Loss:  0.0008400745\n",
            "iteration:  150  Loss:  0.0008400745\n",
            "iteration:  151  Loss:  0.0008400745\n",
            "iteration:  152  Loss:  0.0008400745\n",
            "iteration:  153  Loss:  0.0008400745\n",
            "iteration:  154  Loss:  0.0008400745\n",
            "iteration:  155  Loss:  0.0008400745\n",
            "iteration:  156  Loss:  0.0008400745\n",
            "iteration:  157  Loss:  0.0008400745\n",
            "iteration:  158  Loss:  0.0008400745\n",
            "iteration:  159  Loss:  0.0008400745\n",
            "iteration:  160  Loss:  0.0008400745\n",
            "iteration:  161  Loss:  0.0008400745\n",
            "iteration:  162  Loss:  0.0008400745\n",
            "iteration:  163  Loss:  0.0008400745\n",
            "iteration:  164  Loss:  0.0008400745\n",
            "iteration:  165  Loss:  0.0008400745\n",
            "iteration:  166  Loss:  0.0008400745\n",
            "iteration:  167  Loss:  0.0008400745\n",
            "iteration:  168  Loss:  0.0008400745\n",
            "iteration:  169  Loss:  0.0008400745\n",
            "iteration:  170  Loss:  0.0008400745\n",
            "iteration:  171  Loss:  0.0008400745\n",
            "iteration:  172  Loss:  0.0008400745\n",
            "iteration:  173  Loss:  0.0008400745\n",
            "iteration:  174  Loss:  0.0008400745\n",
            "iteration:  175  Loss:  0.0008400745\n",
            "iteration:  176  Loss:  0.0008400745\n",
            "iteration:  177  Loss:  0.0008400745\n",
            "iteration:  178  Loss:  0.0008400745\n",
            "iteration:  179  Loss:  0.0008400745\n",
            "iteration:  180  Loss:  0.0008400745\n",
            "iteration:  181  Loss:  0.0008400745\n",
            "iteration:  182  Loss:  0.0008400745\n",
            "iteration:  183  Loss:  0.0008400745\n",
            "iteration:  184  Loss:  0.0008400745\n",
            "iteration:  185  Loss:  0.0008400745\n",
            "iteration:  186  Loss:  0.0008400745\n",
            "iteration:  187  Loss:  0.0008400745\n",
            "iteration:  188  Loss:  0.0008400745\n",
            "iteration:  189  Loss:  0.0008400745\n",
            "iteration:  190  Loss:  0.0008400745\n",
            "iteration:  191  Loss:  0.0008400745\n",
            "iteration:  192  Loss:  0.0008400745\n",
            "iteration:  193  Loss:  0.0008400745\n",
            "iteration:  194  Loss:  0.0008400745\n",
            "iteration:  195  Loss:  0.0008400745\n",
            "iteration:  196  Loss:  0.0008400745\n",
            "iteration:  197  Loss:  0.0008400745\n",
            "iteration:  198  Loss:  0.0008400745\n",
            "iteration:  199  Loss:  0.0008400745\n",
            "Total_time_taken: 35.90192198753357\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIGDLzwyCT7d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7926e372-7257-4f0b-d99a-eb9e582af961"
      },
      "source": [
        "start_time = time.time()\n",
        "thetas_optimal, loss_list = rms_prop_optimize(thetas)\n",
        "print('Total_time_taken with jit:',time.time()-start_time)\n",
        "\n",
        "np.save('Trained_Weights.npy',thetas_optimal)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration:  0  Loss:  0.73724437\n",
            "iteration:  1  Loss:  0.53621167\n",
            "iteration:  2  Loss:  0.41269293\n",
            "iteration:  3  Loss:  0.31922403\n",
            "iteration:  4  Loss:  0.24565884\n",
            "iteration:  5  Loss:  0.18742542\n",
            "iteration:  6  Loss:  0.14157689\n",
            "iteration:  7  Loss:  0.105817676\n",
            "iteration:  8  Loss:  0.07823747\n",
            "iteration:  9  Loss:  0.057220656\n",
            "iteration:  10  Loss:  0.041404217\n",
            "iteration:  11  Loss:  0.029645607\n",
            "iteration:  12  Loss:  0.021005502\n",
            "iteration:  13  Loss:  0.014728169\n",
            "iteration:  14  Loss:  0.010222637\n",
            "iteration:  15  Loss:  0.007034554\n",
            "iteration:  16  Loss:  0.0048325686\n",
            "iteration:  17  Loss:  0.0034078665\n",
            "iteration:  18  Loss:  0.0027313998\n",
            "iteration:  19  Loss:  0.0024525223\n",
            "iteration:  20  Loss:  0.001975395\n",
            "iteration:  21  Loss:  0.0014193471\n",
            "iteration:  22  Loss:  0.0010770496\n",
            "iteration:  23  Loss:  0.00090577384\n",
            "iteration:  24  Loss:  0.0008400745\n",
            "iteration:  25  Loss:  0.0008400745\n",
            "iteration:  26  Loss:  0.0008400745\n",
            "iteration:  27  Loss:  0.0008400745\n",
            "iteration:  28  Loss:  0.0008400745\n",
            "iteration:  29  Loss:  0.0008400745\n",
            "iteration:  30  Loss:  0.0008400745\n",
            "iteration:  31  Loss:  0.0008400745\n",
            "iteration:  32  Loss:  0.0008400745\n",
            "iteration:  33  Loss:  0.0008400745\n",
            "iteration:  34  Loss:  0.0008400745\n",
            "iteration:  35  Loss:  0.0008400745\n",
            "iteration:  36  Loss:  0.0008400745\n",
            "iteration:  37  Loss:  0.0008400745\n",
            "iteration:  38  Loss:  0.0008400745\n",
            "iteration:  39  Loss:  0.0008400745\n",
            "iteration:  40  Loss:  0.0008400745\n",
            "iteration:  41  Loss:  0.0008400745\n",
            "iteration:  42  Loss:  0.0008400745\n",
            "iteration:  43  Loss:  0.0008400745\n",
            "iteration:  44  Loss:  0.0008400745\n",
            "iteration:  45  Loss:  0.0008400745\n",
            "iteration:  46  Loss:  0.0008400745\n",
            "iteration:  47  Loss:  0.0008400745\n",
            "iteration:  48  Loss:  0.0008400745\n",
            "iteration:  49  Loss:  0.0008400745\n",
            "iteration:  50  Loss:  0.0008400745\n",
            "iteration:  51  Loss:  0.0008400745\n",
            "iteration:  52  Loss:  0.0008400745\n",
            "iteration:  53  Loss:  0.0008400745\n",
            "iteration:  54  Loss:  0.0008400745\n",
            "iteration:  55  Loss:  0.0008400745\n",
            "iteration:  56  Loss:  0.0008400745\n",
            "iteration:  57  Loss:  0.0008400745\n",
            "iteration:  58  Loss:  0.0008400745\n",
            "iteration:  59  Loss:  0.0008400745\n",
            "iteration:  60  Loss:  0.0008400745\n",
            "iteration:  61  Loss:  0.0008400745\n",
            "iteration:  62  Loss:  0.0008400745\n",
            "iteration:  63  Loss:  0.0008400745\n",
            "iteration:  64  Loss:  0.0008400745\n",
            "iteration:  65  Loss:  0.0008400745\n",
            "iteration:  66  Loss:  0.0008400745\n",
            "iteration:  67  Loss:  0.0008400745\n",
            "iteration:  68  Loss:  0.0008400745\n",
            "iteration:  69  Loss:  0.0008400745\n",
            "iteration:  70  Loss:  0.0008400745\n",
            "iteration:  71  Loss:  0.0008400745\n",
            "iteration:  72  Loss:  0.0008400745\n",
            "iteration:  73  Loss:  0.0008400745\n",
            "iteration:  74  Loss:  0.0008400745\n",
            "iteration:  75  Loss:  0.0008400745\n",
            "iteration:  76  Loss:  0.0008400745\n",
            "iteration:  77  Loss:  0.0008400745\n",
            "iteration:  78  Loss:  0.0008400745\n",
            "iteration:  79  Loss:  0.0008400745\n",
            "iteration:  80  Loss:  0.0008400745\n",
            "iteration:  81  Loss:  0.0008400745\n",
            "iteration:  82  Loss:  0.0008400745\n",
            "iteration:  83  Loss:  0.0008400745\n",
            "iteration:  84  Loss:  0.0008400745\n",
            "iteration:  85  Loss:  0.0008400745\n",
            "iteration:  86  Loss:  0.0008400745\n",
            "iteration:  87  Loss:  0.0008400745\n",
            "iteration:  88  Loss:  0.0008400745\n",
            "iteration:  89  Loss:  0.0008400745\n",
            "iteration:  90  Loss:  0.0008400745\n",
            "iteration:  91  Loss:  0.0008400745\n",
            "iteration:  92  Loss:  0.0008400745\n",
            "iteration:  93  Loss:  0.0008400745\n",
            "iteration:  94  Loss:  0.0008400745\n",
            "iteration:  95  Loss:  0.0008400745\n",
            "iteration:  96  Loss:  0.0008400745\n",
            "iteration:  97  Loss:  0.0008400745\n",
            "iteration:  98  Loss:  0.0008400745\n",
            "iteration:  99  Loss:  0.0008400745\n",
            "iteration:  100  Loss:  0.0008400745\n",
            "iteration:  101  Loss:  0.0008400745\n",
            "iteration:  102  Loss:  0.0008400745\n",
            "iteration:  103  Loss:  0.0008400745\n",
            "iteration:  104  Loss:  0.0008400745\n",
            "iteration:  105  Loss:  0.0008400745\n",
            "iteration:  106  Loss:  0.0008400745\n",
            "iteration:  107  Loss:  0.0008400745\n",
            "iteration:  108  Loss:  0.0008400745\n",
            "iteration:  109  Loss:  0.0008400745\n",
            "iteration:  110  Loss:  0.0008400745\n",
            "iteration:  111  Loss:  0.0008400745\n",
            "iteration:  112  Loss:  0.0008400745\n",
            "iteration:  113  Loss:  0.0008400745\n",
            "iteration:  114  Loss:  0.0008400745\n",
            "iteration:  115  Loss:  0.0008400745\n",
            "iteration:  116  Loss:  0.0008400745\n",
            "iteration:  117  Loss:  0.0008400745\n",
            "iteration:  118  Loss:  0.0008400745\n",
            "iteration:  119  Loss:  0.0008400745\n",
            "iteration:  120  Loss:  0.0008400745\n",
            "iteration:  121  Loss:  0.0008400745\n",
            "iteration:  122  Loss:  0.0008400745\n",
            "iteration:  123  Loss:  0.0008400745\n",
            "iteration:  124  Loss:  0.0008400745\n",
            "iteration:  125  Loss:  0.0008400745\n",
            "iteration:  126  Loss:  0.0008400745\n",
            "iteration:  127  Loss:  0.0008400745\n",
            "iteration:  128  Loss:  0.0008400745\n",
            "iteration:  129  Loss:  0.0008400745\n",
            "iteration:  130  Loss:  0.0008400745\n",
            "iteration:  131  Loss:  0.0008400745\n",
            "iteration:  132  Loss:  0.0008400745\n",
            "iteration:  133  Loss:  0.0008400745\n",
            "iteration:  134  Loss:  0.0008400745\n",
            "iteration:  135  Loss:  0.0008400745\n",
            "iteration:  136  Loss:  0.0008400745\n",
            "iteration:  137  Loss:  0.0008400745\n",
            "iteration:  138  Loss:  0.0008400745\n",
            "iteration:  139  Loss:  0.0008400745\n",
            "iteration:  140  Loss:  0.0008400745\n",
            "iteration:  141  Loss:  0.0008400745\n",
            "iteration:  142  Loss:  0.0008400745\n",
            "iteration:  143  Loss:  0.0008400745\n",
            "iteration:  144  Loss:  0.0008400745\n",
            "iteration:  145  Loss:  0.0008400745\n",
            "iteration:  146  Loss:  0.0008400745\n",
            "iteration:  147  Loss:  0.0008400745\n",
            "iteration:  148  Loss:  0.0008400745\n",
            "iteration:  149  Loss:  0.0008400745\n",
            "iteration:  150  Loss:  0.0008400745\n",
            "iteration:  151  Loss:  0.0008400745\n",
            "iteration:  152  Loss:  0.0008400745\n",
            "iteration:  153  Loss:  0.0008400745\n",
            "iteration:  154  Loss:  0.0008400745\n",
            "iteration:  155  Loss:  0.0008400745\n",
            "iteration:  156  Loss:  0.0008400745\n",
            "iteration:  157  Loss:  0.0008400745\n",
            "iteration:  158  Loss:  0.0008400745\n",
            "iteration:  159  Loss:  0.0008400745\n",
            "iteration:  160  Loss:  0.0008400745\n",
            "iteration:  161  Loss:  0.0008400745\n",
            "iteration:  162  Loss:  0.0008400745\n",
            "iteration:  163  Loss:  0.0008400745\n",
            "iteration:  164  Loss:  0.0008400745\n",
            "iteration:  165  Loss:  0.0008400745\n",
            "iteration:  166  Loss:  0.0008400745\n",
            "iteration:  167  Loss:  0.0008400745\n",
            "iteration:  168  Loss:  0.0008400745\n",
            "iteration:  169  Loss:  0.0008400745\n",
            "iteration:  170  Loss:  0.0008400745\n",
            "iteration:  171  Loss:  0.0008400745\n",
            "iteration:  172  Loss:  0.0008400745\n",
            "iteration:  173  Loss:  0.0008400745\n",
            "iteration:  174  Loss:  0.0008400745\n",
            "iteration:  175  Loss:  0.0008400745\n",
            "iteration:  176  Loss:  0.0008400745\n",
            "iteration:  177  Loss:  0.0008400745\n",
            "iteration:  178  Loss:  0.0008400745\n",
            "iteration:  179  Loss:  0.0008400745\n",
            "iteration:  180  Loss:  0.0008400745\n",
            "iteration:  181  Loss:  0.0008400745\n",
            "iteration:  182  Loss:  0.0008400745\n",
            "iteration:  183  Loss:  0.0008400745\n",
            "iteration:  184  Loss:  0.0008400745\n",
            "iteration:  185  Loss:  0.0008400745\n",
            "iteration:  186  Loss:  0.0008400745\n",
            "iteration:  187  Loss:  0.0008400745\n",
            "iteration:  188  Loss:  0.0008400745\n",
            "iteration:  189  Loss:  0.0008400745\n",
            "iteration:  190  Loss:  0.0008400745\n",
            "iteration:  191  Loss:  0.0008400745\n",
            "iteration:  192  Loss:  0.0008400745\n",
            "iteration:  193  Loss:  0.0008400745\n",
            "iteration:  194  Loss:  0.0008400745\n",
            "iteration:  195  Loss:  0.0008400745\n",
            "iteration:  196  Loss:  0.0008400745\n",
            "iteration:  197  Loss:  0.0008400745\n",
            "iteration:  198  Loss:  0.0008400745\n",
            "iteration:  199  Loss:  0.0008400745\n",
            "Total_time_taken with jit: 4.442589044570923\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcjGOkgT9zsI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}